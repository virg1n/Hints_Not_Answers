{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbe7e40-102d-4eec-835e-3110882d164c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b241ff5375f4065acd2fb73bd916140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B\"\n",
    "CHECKPOINT_DIR = \"qwen2.5_7B_sft/checkpoint-1-8000\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CHECKPOINT_DIR,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28ea9faa-dece-456f-947c-929ef110e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "file_path = 'halterman17.pdf' #halterman17\n",
    "text = \"\"\n",
    "with open(file_path, 'rb') as f:\n",
    "    reader = PyPDF2.PdfReader(f)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "def chunk_text(text, chunk_size=100, overlap=30):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        chunk = words[start : start + chunk_size]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "def retrieve_chunks(query, chunks, chunk_embeddings, top_k=1):\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    cos_scores = torch.nn.functional.cosine_similarity(query_embedding, chunk_embeddings)\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "    top_chunks = [chunks[idx] for idx in top_results.indices]\n",
    "    return top_chunks\n",
    "\n",
    "def generate_answer(query, max_new_tokens=200):\n",
    "    retrieved_chunks = retrieve_chunks(query, chunks, chunk_embeddings, top_k=1)\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>context\\n{context}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    # print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.3)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # answer = answer.split(\"Question: \")[1]\n",
    "    return answer\n",
    "\n",
    "# query = \"whats name of the model? How many parameters its use?\"\n",
    "# answer = generate_answer(query)\n",
    "# print(\"\\nGenerated Answer:\")\n",
    "# print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfa69998-e6dd-4d45-bb3e-358fe74d5139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Write a function length_of_last_word(text) that takes as an argument a string text, and returns the length of the last word in the string. You may assume text is a non-empty English text where words are separated by whitespace characters (including \n",
      " \t \f\n",
      " and spaces). There is no need to handle punctuations differently from other characters.\n",
      "context\n",
      "it in a different way. The following shows the preferred way of determining a stringâ€™s length: >>> s /quotesingle.VarABCEFGHI /quotesingle.Var >>> s = /quotesingle.VarABCEFGHI /quotesingle.Var >>> s /quotesingle.VarABCEFGHI /quotesingle.Var >>> len(s) 8 >>> s.__len__() 8 The expressions len(s) ands.__len__() are functionally equivalent. Instead of calling the __len__ method directly, clients should use the global lenfunction. Listing 9.4 ( printcharacters.py ) uses the len function and []index operator to print the individual characters that make up a string. Listing 9.4: printcharacters.py s = /quotedbl.VarABCDEFGHIJK/quotedbl.Var print(s) for i in range(len(s)): print(/quotedbl.Var[/quotedbl.Var, s[i], /quotedbl.Var]/quotedbl.Var, sep=/quotedbl.Var/quotedbl.Var, end=/quotedbl.Var/quotedbl.Var) print() # Print newline for ch in\n",
      "assistant\n",
      "To solve this problem, you can follow these steps:\n",
      "\n",
      "1. Split the input string into individual words.\n",
      "2. Initialize a counter variable to zero.\n",
      "3. Loop through each word in the list of characters.\n",
      "4. For each word, check its length against the current longest word's length.\n",
      "5. If the current word is longer than the longest word in the list, update the longest word.\n",
      "6. After completing the loop, return the longest word.\n",
      "\n",
      "Remember to handle the initial case where the longest word is empty or the longest word is empty. This can be done by iterating through the list while iterating through the longest word, or by using copy copy copies of the copy of the longest word to avoid any duplicate characters.\n"
     ]
    }
   ],
   "source": [
    "query = \"Write a function length_of_last_word(text) that takes as an argument a string text, and returns the length of the last word in the string. You may assume text is a non-empty English text where words are separated by whitespace characters (including \\n \\r \\t \\f and spaces). There is no need to handle punctuations differently from other characters.\"\n",
    "answer = generate_answer(query)\n",
    "# print(\"\\nGenerated Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a4fd0-8319-471e-8a7b-b881b03360db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import PyPDF2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "CACHE_FILE = 'tokenized_medium.pkl'\n",
    "CHUNK_SIZE = 500\n",
    "OVERLAP = 50\n",
    "\n",
    "\n",
    "def load_or_tokenize_pdf(pdf_path: str, cache_path: str = CACHE_FILE, chunk_size: int = CHUNK_SIZE, overlap: int = OVERLAP):\n",
    "    if os.path.exists(cache_path):\n",
    "        with open(cache_path, 'rb') as f:\n",
    "            chunks = pickle.load(f)\n",
    "        print(f\"Loaded {len(chunks)} chunks from cache '{cache_path}'\")\n",
    "    \n",
    "    else:\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            for page in reader.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            chunk = words[start : start + chunk_size]\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            start += (chunk_size - overlap)\n",
    "\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(chunks, f)\n",
    "        print(f\"Tokenized and saved {len(chunks)} chunks to cache '{cache_path}'\")\n",
    "    return chunks\n",
    "\n",
    "chunks = load_or_tokenize_pdf('medium.pdf')\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "\n",
    "def retrieve_chunks(query: str, chunks: list, chunk_embeddings: torch.Tensor, top_k: int = 5) -> list:\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "    cos_scores = torch.nn.functional.cosine_similarity(query_embedding, chunk_embeddings)\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "    return [chunks[idx] for idx in top_results.indices]\n",
    "\n",
    "\n",
    "def generate_answer(query, max_new_tokens=200):\n",
    "    retrieved_chunks = retrieve_chunks(query, chunks, chunk_embeddings, top_k=3)\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>context\\n{context}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.7)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # answer = answer.split(\"Question: \")[1]\n",
    "    return answer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    query = \"What's the name of the model? How many parameters does it use?\"\n",
    "    answer = generate_answer(query)\n",
    "    print(\"\\nGenerated Answer:\")\n",
    "    print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
